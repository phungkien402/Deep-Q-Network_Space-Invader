"""
DQN Training cho Space Invaders - ƒê·ªì √Ån T·ªët Nghi·ªáp
===================================================
C√†i ƒë·∫∑t Double DQN v·ªõi c√°c t√≠nh nƒÉng ch√≠nh:
- Frame Stacking: X·∫øp ch·ªìng 4 khung h√¨nh li√™n ti·∫øp ƒë·ªÉ hi·ªÉu chuy·ªÉn ƒë·ªông
- Frame Skipping: L·∫∑p l·∫°i m·ªói h√†nh ƒë·ªông 4 l·∫ßn (chu·∫©n DeepMind)
- Reward Shaping: ƒêi·ªÅu ch·ªânh ph·∫ßn th∆∞·ªüng ƒë·ªÉ h∆∞·ªõng d·∫´n agent h·ªçc t·ªët h∆°n
- Experience Replay: L∆∞u v√† h·ªçc l·∫°i t·ª´ kinh nghi·ªám qu√° kh·ª©
- Target Network: M·∫°ng ri√™ng bi·ªát ƒë·ªÉ ·ªïn ƒë·ªãnh Q-values
- Epsilon-Greedy: C√¢n b·∫±ng gi·ªØa kh√°m ph√° v√† khai th√°c

T√°c gi·∫£: [T√™n c·ªßa b·∫°n]
Ng√†y: Th√°ng 10/2025
"""

import numpy as np
import matplotlib.pyplot as plt
import pickle
from collections import deque
import gymnasium as gym
import ale_py
from gymnasium import wrappers
import torch
import signal
import sys
import random
import time
from agent import DQNAgent, FrameStack, DEVICE

# ƒêƒÉng k√Ω m√¥i tr∆∞·ªùng ALE (Arcade Learning Environment)
gym.register_envs(ale_py)

# =========================================================================
# KH·ªûI T·∫†O RANDOM SEED: ƒê·∫£m b·∫£o m·ªói l·∫ßn ch·∫°y c√≥ k·∫øt qu·∫£ kh√°c nhau
# =========================================================================
# D√πng timestamp ƒë·ªÉ t·∫°o seed kh√°c nhau m·ªói l·∫ßn train
# ƒêi·ªÅu n√†y ngƒÉn h√†nh vi deterministic v√† ƒë·∫£m b·∫£o ƒëa d·∫°ng trong training
random_seed = int(time.time() * 1000) % 2**32
random.seed(random_seed)
np.random.seed(random_seed)
torch.manual_seed(random_seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)
print(f"üé≤ Random seed: {random_seed}")

# =========================================================================
# TH√îNG TIN H·ªÜ TH·ªêNG: Ki·ªÉm tra GPU c√≥ s·∫µn kh√¥ng
# =========================================================================
print("=" * 60)
print("TH√îNG TIN H·ªÜ TH·ªêNG")
print("=" * 60)
print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print(f"Current Device: {torch.cuda.current_device()}")
else:
    print("‚ö†Ô∏è  C·∫¢NH B√ÅO: KH√îNG C√ì CUDA - TRAINING TR√äN CPU!")
print("=" * 60)
print()


# =========================================================================
# H√ÄM TRAINING
# =========================================================================
def train(n_episodes=200,
          max_t=10000,
          eps_start=1.0,  # B·∫Øt ƒë·∫ßu v·ªõi 100% kh√°m ph√°
          eps_end=0.01,   # T·ªëi thi·ªÉu 1% kh√°m ph√° ƒë·ªÉ tr√°nh b·ªã stuck
          eps_decay=0.995,  # T·ªëc ƒë·ªô gi·∫£m epsilon (~5% sau 500 episodes)
          start_episode=0,  # Episode b·∫Øt ƒë·∫ßu (ƒë·ªÉ ti·∫øp t·ª•c training)
          update_every=4,  # H·ªçc m·ªói 4 b∆∞·ªõc (l·ª±a ch·ªçn c·ªßa DeepMind)
          hyperparams=None):  # Tham s·ªë cho vi·ªác l∆∞u checkpoint
    """
    Training agent Deep Q-Learning ƒë·ªÉ ch∆°i Space Invaders
    ---
    Tham s·ªë
    =======
        n_episodes (int): S·ªë l∆∞·ª£ng episodes t·ªëi ƒëa ƒë·ªÉ train
        max_t (int): S·ªë timesteps t·ªëi ƒëa m·ªói episode
        eps_start (float): Gi√° tr·ªã epsilon b·∫Øt ƒë·∫ßu (kh√°m ph√°)
        eps_end (float): Gi√° tr·ªã epsilon t·ªëi thi·ªÉu
        eps_decay (float): H·ªá s·ªë gi·∫£m epsilon m·ªói episode
        start_episode (int): Episode b·∫Øt ƒë·∫ßu (khi ti·∫øp t·ª•c training)
    Tr·∫£ v·ªÅ
    ======
        scores: Danh s√°ch ƒëi·ªÉm s·ªë c·ªßa m·ªói episode
        done_timesteps: Danh s√°ch s·ªë b∆∞·ªõc c·ªßa m·ªói episode
    """
    # L∆∞u ƒëi·ªÉm s·ªë c·ªßa m·ªói episode
    scores = []

    # L∆∞u s·ªë timesteps c·ªßa m·ªói episode khi game k·∫øt th√∫c
    done_timesteps = []

    # 100 ƒëi·ªÉm g·∫ßn nh·∫•t d√πng ƒë·ªÉ t√≠nh ƒëi·ªÉm trung b√¨nh
    scores_window = deque(maxlen=100)
    
    # Theo d√µi loss trong qu√° tr√¨nh training
    losses = []
    
    eps = eps_start
    
    # =========================================================================
    # X·ª¨ L√ù CTRL+C: Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ti·∫øn tr√¨nh khi d·ª´ng training
    # =========================================================================
    def signal_handler(sig, frame):
        print('\n\nüõë Training b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng (Ctrl+C)')
        print(f'üìä ƒêang v·∫Ω bi·ªÉu ƒë·ªì cho {len(scores)} episodes...')
        
        if len(scores) >= 100:
            # V·∫Ω running mean (trung b√¨nh tr∆∞·ª£t)
            N = 100
            cumulative_timesteps = np.cumsum(done_timesteps)
            smoothed_scores = np.convolve(np.array(scores), np.ones((N,)) / N, mode='valid')
            smoothed_timesteps = np.convolve(cumulative_timesteps, np.ones((N,)) / N, mode='valid')
            
            fig = plt.figure()
            plt.plot(smoothed_timesteps, smoothed_scores)
            plt.ylabel('ƒêi·ªÉm S·ªë')
            plt.xlabel('Timesteps')
            plt.title('DQN Training - Trung B√¨nh 100 Episodes (B·ªã D·ª´ng)')
            plt.show()
        else:
            print(f'‚ö†Ô∏è Ch∆∞a ƒë·ªß episodes ({len(scores)}) ƒë·ªÉ t√≠nh trung b√¨nh 100 episodes')
        
        sys.exit(0)
    
    # ƒêƒÉng k√Ω signal handler
    signal.signal(signal.SIGINT, signal_handler)
    
    # =========================================================================
    # FRAME STACKING: X·∫øp ch·ªìng 4 khung h√¨nh ƒë·ªÉ cung c·∫•p th√¥ng tin th·ªùi gian
    # =========================================================================
    # X·∫øp ch·ªìng 4 khung h√¨nh li√™n ti·∫øp ƒë·ªÉ agent c√≥ th·ªÉ nh·∫≠n bi·∫øt chuy·ªÉn ƒë·ªông v√† v·∫≠n t·ªëc
    # ƒêi·ªÅu n√†y quan tr·ªçng cho game Atari v√¨ 1 khung h√¨nh kh√¥ng th·ªÉ hi·ªán chuy·ªÉn ƒë·ªông
    frame_stack = FrameStack(num_frames=4)
    
    # Debug: Ki·ªÉm tra vi·ªác random c√≥ ho·∫°t ƒë·ªông kh√¥ng
    if start_episode == 0:
        print(f"üîç 5 gi√° tr·ªã random ƒë·∫ßu ti√™n: {[np.random.rand() for _ in range(5)]}")
    
    # =========================================================================
    # V√íNG L·∫∂P TRAINING CH√çNH: L·∫∑p qua c√°c episodes
    # =========================================================================
    for i_episode in range(start_episode + 1, start_episode + n_episodes + 1):
        # Reset m√¥i tr∆∞·ªùng v·ªõi seed ng·∫´u nhi√™n ƒë·ªÉ ƒëa d·∫°ng
        episode_seed = np.random.randint(0, 2**31 - 1)
        raw_state, info = env.reset(seed=episode_seed)
        
        # Kh·ªüi t·∫°o frame stack v·ªõi observation ƒë·∫ßu ti√™n
        processed_frame = agent.preprocess_state(raw_state).cpu().numpy().squeeze()
        frame_stack.reset(processed_frame)
        state = torch.from_numpy(np.stack(frame_stack.frames, axis=0)).float().unsqueeze(0).to(DEVICE)
        
        # Bi·∫øn theo d√µi episode
        score = 0  # T·ªïng ph·∫ßn th∆∞·ªüng c·ªßa episode n√†y
        episode_experiences = []  # L∆∞u t·∫•t c·∫£ experiences ƒë·ªÉ replay
        prev_lives = info.get('lives', 3)  # Theo d√µi s·ªë m·∫°ng ƒë·ªÉ ph·∫°t khi ch·∫øt
        prev_action = None  # Theo d√µi ƒëa d·∫°ng h√†nh ƒë·ªông
        action_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}  # Ph√¢n ph·ªëi h√†nh ƒë·ªông
        episode_losses = []  # Theo d√µi training loss
        
        # =====================================================================
        # V√íNG L·∫∂P EPISODE: T∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng v√† h·ªçc
        # =====================================================================
        for timestep in range(max_t):
            # CH·ªåN H√ÄNH ƒê·ªòNG: Epsilon-greedy policy
            action = agent.act(state, eps)
            
            # TH·ª∞C HI·ªÜN H√ÄNH ƒê·ªòNG: B∆∞·ªõc trong m√¥i tr∆∞·ªùng
            raw_next_state, reward, done, truncated, info = env.step(action)
            done = done or truncated
            
            # X·ª¨ L√ù OBSERVATION: √Åp d·ª•ng frame stacking
            processed_next_frame = agent.preprocess_state(raw_next_state).cpu().numpy().squeeze()
            frame_stack.add(processed_next_frame)
            next_state = torch.from_numpy(np.stack(frame_stack.frames, axis=0)).float().unsqueeze(0).to(DEVICE)
            
            # Theo d√µi ph√¢n ph·ªëi h√†nh ƒë·ªông ƒë·ªÉ ph√¢n t√≠ch
            action_counts[action] += 1
            
            # =====================================================================
            # REWARD SHAPING: Thi·∫øt k·∫ø ph·∫ßn th∆∞·ªüng ƒëa m·ª•c ti√™u
            # =====================================================================
            # Ph·∫ßn th∆∞·ªüng c∆° b·∫£n t·ª´ m√¥i tr∆∞·ªùng (ƒëi·ªÉm t·ª´ vi·ªác ti√™u di·ªát k·∫ª ƒë·ªãch)
            shaped_reward = reward
            current_lives = info.get('lives', 0)
            
            # 1. PH·∫†T M·∫§T M·∫†NG: T√≠n hi·ªáu √¢m m·∫°nh khi agent ch·∫øt
            #    Gi√∫p agent h·ªçc c√°ch tr√°nh ƒë·∫°n ƒë·ªãch
            if current_lives < prev_lives:
                shaped_reward -= 5.0  # Ph·∫°t ƒë√°ng k·ªÉ ƒë·ªÉ ∆∞u ti√™n s·ªëng s√≥t
                print(f"    üíî M·∫•t m·∫°ng! M·∫°ng c√≤n l·∫°i: {current_lives} (Episode {i_episode}, B∆∞·ªõc {timestep})")
            
            # 2. TH∆Ø·ªûNG S·ªêNG S√ìT: Ph·∫ßn th∆∞·ªüng nh·ªè cho vi·ªác s·ªëng s√≥t
            #    Khuy·∫øn kh√≠ch episodes d√†i h∆°n v√† nhi·ªÅu c∆° h·ªôi h·ªçc h∆°n
            if not done:
                shaped_reward += 0.01
            
            # 3. TH∆Ø·ªûNG B·∫ÆN: Khuy·∫øn kh√≠ch ch∆°i t·∫•n c√¥ng
            #    Space Invaders c·∫ßn b·∫Øn ƒë·ªÉ ghi ƒëi·ªÉm, ƒëi·ªÅu n√†y h∆∞·ªõng d·∫´n agent
            if action in [1, 4, 5]:  # FIRE, RIGHTFIRE, LEFTFIRE
                shaped_reward += 0.05
            
            # 4. TH∆Ø·ªûNG DI CHUY·ªÇN: Khuy·∫øn kh√≠ch t√¨m v·ªã tr√≠ chi·∫øn thu·∫≠t
            #    Agent c·∫ßn n√© tr√°nh v√† di chuy·ªÉn ƒë·ªÉ tr√°nh ƒë·∫°n ƒë·ªãch
            if action in [2, 3]:  # RIGHT, LEFT (di chuy·ªÉn ngang)
                shaped_reward += 0.03
            
            # 5. PH·∫†T KH√îNG L√ÄM G√å: Kh√¥ng khuy·∫øn kh√≠ch ƒë·ª£i qu√° nhi·ªÅu
            #    NgƒÉn agent h·ªçc c√°c chi·∫øn l∆∞·ª£c th·ª• ƒë·ªông
            if action == 0:  # NOOP (kh√¥ng l√†m g√¨)
                shaped_reward -= 0.01

            prev_lives = current_lives
            prev_action = action
            
            # =====================================================================
            # EXPERIENCE REPLAY: Store and learn from experience
            # =====================================================================
            # Store experience tuple (s, a, r, s', done) for later replay
            episode_experiences.append((state, action, shaped_reward, next_state, done))
            
            # LEARN: Update Q-network using experience replay
            loss = agent.step(state, action, shaped_reward, next_state, done)
            if loss is not None:
                episode_losses.append(loss)
            
            # ‚ö° Aggressive memory cleanup for 4GB GPU
            if timestep % 100 == 0:
                torch.cuda.empty_cache()
            
            state = next_state
            score += reward  # Score v·∫´n d√πng reward g·ªëc ƒë·ªÉ ƒë√°nh gi√°
            if done:
                done_timesteps.append(timestep)
                break

        # Append score AFTER episode ends
        scores_window.append(score)
        scores.append(score)
        
        # Calculate average score
        avg_score = np.mean(scores_window) if len(scores_window) > 0 else 0
        
        # Calculate average loss for this episode
        avg_loss = np.mean(episode_losses) if len(episode_losses) > 0 else 0
        if len(episode_losses) > 0:
            losses.append(avg_loss)
        
        # Print action distribution every 50 episodes
        if i_episode % 1 == 0:
            left_usage = action_counts[3]
            right_usage = action_counts[2]
            right_fire_usage = action_counts[4]
            left_fire_usage = action_counts[5]
            fire_usage = action_counts[1]
            noop_usage = action_counts[0]
            total_actions = sum(action_counts.values())

            print(f"    üìä Action stats: LEFT={left_usage}, RIGHT={right_usage}, LEFT_FIRE={left_fire_usage}, RIGHT_FIRE={right_fire_usage}, FIRE={fire_usage}, NOOP={noop_usage}, TOTAL={total_actions}")

        # Decrease epsilon
        eps = max(eps * eps_decay, eps_end)
        
        # Print episode summary with loss and total timesteps
        loss_str = f'Loss: {avg_loss:.4f}' if avg_loss > 0 else 'Loss: N/A'
        total_steps = agent.total_steps
        warm_up_status = f' üî• LEARNING!' if total_steps >= agent.learning_starts else f' ‚è≥ Warm-up: {total_steps}/{agent.learning_starts}'
        print('Episode {}\tScore: {:.2f}\tAvg: {:.2f}\t{}\tEps: {:.3f}\tSteps: {}{}'.format(
            i_episode, score, avg_score, loss_str, eps, total_steps, warm_up_status))
        
        # ‚ö° Cleanup episode data and GPU cache (4GB GPU)
        del episode_experiences, episode_losses
        torch.cuda.empty_cache()
        
        # ‚ö° Aggressive GPU memory cleanup every 10 episodes
        if i_episode % 10 == 0:
            # Clear optimizer state to free memory
            agent.optimizer.zero_grad(set_to_none=True)
            torch.cuda.empty_cache()
            # Print memory usage
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1e9
                reserved = torch.cuda.memory_reserved(0) / 1e9
                print(f"    üßπ GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")
        
        # Save model every SAVE_EVERY episodes
        if i_episode % SAVE_EVERY == 0:
            # Save model weights v·ªõi t√™n ri√™ng theo episode
            checkpoint_name = f'model_ep{i_episode}.pth'
            torch.save(agent.qnetwork_local.state_dict(),
                       SAVE_DIR + checkpoint_name)
            print(f'üíæ Checkpoint saved: {checkpoint_name}')
            
            # ‚ö° Save COMPREHENSIVE training state for thesis reporting
            training_state = {
                # Training progress
                'epsilon': eps,
                'episode': i_episode,
                'total_steps': agent.total_steps,
                
                # Performance metrics
                'scores': scores,
                'done_timesteps': done_timesteps,
                'losses': losses,  # Average loss per episode
                
                # Statistics for reporting
                'avg_score_last_100': np.mean(scores_window) if len(scores_window) > 0 else 0,
                'max_score': np.max(scores) if len(scores) > 0 else 0,
                'min_score': np.min(scores) if len(scores) > 0 else 0,
                'avg_loss': np.mean(losses[-100:]) if len(losses) >= 100 else (np.mean(losses) if len(losses) > 0 else 0),
                
                # Hyperparameters (for reproducibility)
                'hyperparameters': {
                    'buffer_size': hyperparams['buffer_size'] if hyperparams else 'unknown',
                    'batch_size': agent.batch_size,
                    'gamma': agent.gamma,
                    'lr': hyperparams['lr'] if hyperparams else 'unknown',
                    'tau': agent.tau,
                    'update_every': agent.update_every,
                    'learning_starts': agent.learning_starts,
                    'eps_start': eps_start,
                    'eps_end': eps_end,
                    'eps_decay': eps_decay,
                    'frameskip': 4  # Document this important parameter
                },
                
                # Timestamps
                'saved_at': time.strftime("%Y-%m-%d %H:%M:%S"),
                'random_seed': random_seed
            }
            state_name = f'training_state_ep{i_episode}.pkl'
            with open(SAVE_DIR + state_name, 'wb') as fp:
                pickle.dump(training_state, fp)
            
            print(f'üìä Stats: Avg={training_state["avg_score_last_100"]:.2f}, Max={training_state["max_score"]:.0f}, Avg Loss={training_state["avg_loss"]:.4f}')

    # save the final network
    torch.save(agent.qnetwork_local.state_dict(), SAVE_DIR + 'model.pth')
    
    # ‚ö° Save COMPREHENSIVE final training state
    training_state = {
        # Training progress
        'epsilon': eps,
        'episode': start_episode + n_episodes,
        'total_steps': agent.total_steps,
        
        # Performance metrics
        'scores': scores,
        'done_timesteps': done_timesteps,
        'losses': losses,
        
        # Statistics for reporting
        'avg_score_last_100': np.mean(scores_window) if len(scores_window) > 0 else 0,
        'max_score': np.max(scores) if len(scores) > 0 else 0,
        'min_score': np.min(scores) if len(scores) > 0 else 0,
        'avg_loss': np.mean(losses[-100:]) if len(losses) >= 100 else (np.mean(losses) if len(losses) > 0 else 0),
        
        # Hyperparameters
        'hyperparameters': {
            'buffer_size': hyperparams['buffer_size'] if hyperparams else 'unknown',
            'batch_size': agent.batch_size,
            'gamma': agent.gamma,
            'lr': hyperparams['lr'] if hyperparams else 'unknown',
            'tau': agent.tau,
            'update_every': agent.update_every,
            'learning_starts': agent.learning_starts,
            'eps_start': eps_start,
            'eps_end': eps_end,
            'eps_decay': eps_decay,
            'frameskip': 4
        },
        
        # Timestamps
        'saved_at': time.strftime("%Y-%m-%d %H:%M:%S"),
        'random_seed': random_seed
    }
    with open(SAVE_DIR + 'training_state.pkl', 'wb') as fp:
        pickle.dump(training_state, fp)
    
    print(f'\n{"="*60}')
    print(f'üìä FINAL TRAINING STATISTICS')
    print(f'{"="*60}')
    print(f'Total Episodes: {start_episode + n_episodes}')
    print(f'Total Steps: {agent.total_steps:,}')
    print(f'Avg Score (last 100): {training_state["avg_score_last_100"]:.2f}')
    print(f'Max Score: {training_state["max_score"]:.0f}')
    print(f'Min Score: {training_state["min_score"]:.0f}')
    print(f'Avg Loss: {training_state["avg_loss"]:.4f}')
    print(f'Final Epsilon: {eps:.4f}')
    print(f'{"="*60}\n')

    # save the final scores (for backward compatibility)
    with open(SAVE_DIR + 'scores', 'wb') as fp:
        pickle.dump(scores, fp)

    # save the done timesteps (for backward compatibility)
    with open(SAVE_DIR + 'dones', 'wb') as fp:
        pickle.dump(done_timesteps, fp)

    return scores, done_timesteps


def test(env, trained_agent, n_games=5, n_steps_per_game=10000):
    # ‚ö° FRAME STACKING for test mode
    frame_stack = FrameStack(num_frames=4)
    
    for game in range(n_games):
        env = wrappers.RecordVideo(env,
                               "./test/game-{}".format(game),
                               episode_trigger=lambda x: True)

        raw_observation, _ = env.reset()
        
        # Initialize frame stack
        processed_frame = trained_agent.preprocess_state(raw_observation).cpu().numpy().squeeze()
        frame_stack.reset(processed_frame)
        observation = torch.from_numpy(np.stack(frame_stack.frames, axis=0)).float().unsqueeze(0).to(DEVICE)
        
        score = 0
        action_counts = [0] * env.action_space.n  # Track action distribution
        debug_step = 0
        for step in range(n_steps_per_game):
            # Debug first few steps
            debug = (debug_step < 3)
            action = trained_agent.act(observation, eps=0.0, debug=debug)  # eps=0 ƒë·ªÉ kh√¥ng random
            action_counts[action] += 1
            
            if debug_step < 3:
                print(f"  Step {debug_step}: action={action}")
                debug_step += 1
            
            raw_observation, reward, done, truncated, info = env.step(action)
            
            # Process next frame with stacking
            processed_frame = trained_agent.preprocess_state(raw_observation).cpu().numpy().squeeze()
            frame_stack.add(processed_frame)
            observation = torch.from_numpy(np.stack(frame_stack.frames, axis=0)).float().unsqueeze(0).to(DEVICE)
            
            score += reward
            if done or truncated:
                print('GAME-{} OVER! score={}'.format(game, score))
                print(f'  Action distribution: {action_counts}')
                break
        env.close()


# Agent was trained on GPU in colab.
# The files presented in train folder are those colab
# TODO
# - Encapsulate the training data into a trainloader to avoid GPU runtime error

if __name__ == '__main__':
    TRAIN = True  # ‚ö° TRAIN mode with frame stacking!
    BUFFER_SIZE = 20000  # ‚ö° REDUCED for 4GB GPU (lower for safety)
    BATCH_SIZE = 32  # ‚ö° REDUCED to 24 for 4GB GPU (was 32)
    GAMMA = 0.90  # discount factor
    TAU = 1e-3  # for soft update of target parameters
    LR = 0.0001  # learning rate 1e-4 (document recommended)
    UPDATE_EVERY = 16  # Learn every 16 steps (less frequent for 4GB GPU)
    SAVE_EVERY = 50  # ‚ö° Save th∆∞·ªùng xuy√™n h∆°n (m·ªói 50 episodes)
    MAX_TIMESTEPS = 10000  # max timesteps m·ªói episode
    N_EPISODES = 1000  # ‚ö° Train th√™m 2000 episodes
    SAVE_DIR = "./train/"  # L∆∞u v√†o folder train duy nh·∫•t
    LOAD_MODEL = True   # ‚ö° TRUE: Continue training from ep2000 with epsilon=0.01

    if LOAD_MODEL:
        LEARNING_STARTS = 0
    else:
        LEARNING_STARTS = 10000  # ‚ö° NO WARM-UP when continuing training (was 10000)

    # Create environment with render_mode for recording videos in test mode
    # ‚ö° FRAMESKIP: Repeat each action for 4 frames (DeepMind standard)
    # This makes agent move faster (like in demo videos) and speeds up training 4x
    if TRAIN:
        env = gym.make('SpaceInvaders-v4', frameskip=4)
    else:
        env = gym.make('SpaceInvaders-v4', render_mode='rgb_array', frameskip=4)

    if TRAIN:
        # init agent
        agent = DQNAgent(state_size=4,
                         action_size=env.action_space.n,
                         seed=0,
                         lr=LR,
                         gamma=GAMMA,
                         tau=TAU,
                         buffer_size=BUFFER_SIZE,
                         batch_size=BATCH_SIZE,
                         update_every=UPDATE_EVERY,
                         learning_starts=LEARNING_STARTS)  # SB3: Warm-up period
        
        # Load model ƒë√£ train tr∆∞·ªõc (n·∫øu c√≥)
        loaded_epsilon = None
        loaded_episode = 0
        if LOAD_MODEL:
            try:
                # ‚ö° AUTO-FIND latest checkpoint
                import glob
                checkpoint_files = glob.glob(SAVE_DIR + 'model_ep*.pth')
                if checkpoint_files:
                    # Sort by episode number (extract from filename)
                    checkpoint_files.sort(key=lambda x: int(x.split('_ep')[1].split('.pth')[0]))
                    latest_checkpoint = checkpoint_files[-1]
                    episode_num = int(latest_checkpoint.split('_ep')[1].split('.pth')[0])
                    
                    checkpoint_path = latest_checkpoint
                    state_path = SAVE_DIR + f'training_state_ep{episode_num}.pkl'
                else:
                    # Fallback to generic files
                    checkpoint_path = SAVE_DIR + 'model.pth'
                    state_path = SAVE_DIR + 'training_state.pkl'
                
                agent.qnetwork_local.load_state_dict(torch.load(checkpoint_path))
                agent.qnetwork_target.load_state_dict(torch.load(checkpoint_path))
                print(f"‚úÖ Loaded model from {checkpoint_path}")
                
                # Load training state (epsilon, episode count, etc.)
                try:
                    with open(state_path, 'rb') as fp:
                        training_state = pickle.load(fp)
                        loaded_epsilon = training_state.get('epsilon', None)
                        loaded_episode = training_state.get('episode', 0)
                        
                        # ‚ö° ASK USER: Adjust epsilon?
                        print(f"\n{'='*60}")
                        print(f"üìä CURRENT TRAINING STATE")
                        print(f"{'='*60}")
                        print(f"Episode: {loaded_episode}")
                        print(f"Current Epsilon: {loaded_epsilon:.4f}")
                        print(f"Avg Score (last 100): {np.mean(training_state['scores'][-100:]):.2f}")
                        print(f"{'='*60}\n")
                        
                        response = input("üîç Change epsilon? (y/n) [default: n]: ").strip().lower()
                        
                        if response == 'y':
                            try:
                                new_epsilon = float(input(f"Enter new epsilon value (current: {loaded_epsilon:.4f}): "))
                                if 0 <= new_epsilon <= 1:
                                    loaded_epsilon = new_epsilon
                                    print(f"‚úÖ Epsilon set to: {new_epsilon:.4f}")
                                else:
                                    print(f"‚ö†Ô∏è Invalid epsilon (must be 0-1), keeping {loaded_epsilon:.4f}")
                            except:
                                print(f"‚ö†Ô∏è Invalid input, keeping epsilon at {loaded_epsilon:.4f}")
                        else:
                            print(f"‚úÖ Keeping epsilon at {loaded_epsilon:.4f}")
                        
                        # ‚ö° CRITICAL: Restore total_steps from saved state (not estimate!)
                        saved_steps = training_state.get('total_steps', None)
                        if saved_steps is not None:
                            agent.total_steps = saved_steps
                            print(f"‚úÖ Restored total_steps to: {saved_steps:,}")
                        else:
                            # Fallback to estimate if old checkpoint without total_steps
                            estimated_steps = loaded_episode * 1000
                            agent.total_steps = estimated_steps
                            print(f"‚ö†Ô∏è Estimated total_steps to: {estimated_steps:,} (old checkpoint)")
                        
                        if loaded_epsilon:
                            print(f"‚úÖ Epsilon set to: {loaded_epsilon:.4f} (was {training_state.get('epsilon', 0):.4f})")
                        if loaded_episode:
                            print(f"‚úÖ Continuing from episode: {loaded_episode}")
                except:
                    print("‚ö†Ô∏è No training state found, using default epsilon")
            except:
                print("‚ö†Ô∏è No checkpoint found, starting from scratch")
        
        # Prepare hyperparameters dict for checkpoint saving
        hyperparams_dict = {
            'buffer_size': BUFFER_SIZE,
            'batch_size': BATCH_SIZE,
            'gamma': GAMMA,
            'lr': LR,
            'tau': TAU,
            'update_every': UPDATE_EVERY,
            'learning_starts': LEARNING_STARTS
        }
        
        # train and get the scores
        # Pass loaded epsilon and episode to continue from where we left off
        if loaded_epsilon is not None:
            scores, done_timesteps = train(n_episodes=N_EPISODES, max_t=MAX_TIMESTEPS, 
                          eps_start=loaded_epsilon, start_episode=loaded_episode,
                          hyperparams=hyperparams_dict)
        else:
            scores, done_timesteps = train(n_episodes=N_EPISODES, max_t=MAX_TIMESTEPS,
                          hyperparams=hyperparams_dict)

        # plot the running mean of scores
        N = 100  # running mean window
        fig = plt.figure()
        ax = fig.add_subplot(111)
        
        # Calculate cumulative timesteps for each episode
        cumulative_timesteps = np.cumsum(done_timesteps)
        
        # Apply running mean to both scores and timesteps
        smoothed_scores = np.convolve(np.array(scores), np.ones((N, )) / N, mode='valid')
        smoothed_timesteps = np.convolve(cumulative_timesteps, np.ones((N, )) / N, mode='valid')
        
        plt.plot(smoothed_timesteps, smoothed_scores)
        plt.ylabel('Score')
        plt.xlabel('Timesteps')
        plt.title('DQN Training - 100-Episode Running Mean')
        plt.show()
    else:
        N_GAMES = 5
        N_STEPS_PER_GAME = 10000

        # init a new agent
        trained_agent = DQNAgent(state_size=4,
                                 action_size=env.action_space.n,
                                 seed=0)

        # ‚ö†Ô∏è COMMENTED OUT: Old model incompatible with frame stacking (1 channel ‚Üí 4 channels)
        # # replace the weights with the trained weights
        # # Load latest checkpoint (model_ep550)
        # model_path = "./train/model/model_ep550.pth"
        # print(f"Loading model from {model_path}...")
        # trained_agent.qnetwork_local.load_state_dict(
        #     torch.load(model_path))
        
        # # Verify model loaded correctly by checking some weights
        # print("‚úÖ Model loaded. First layer weight sum:", 
        #       trained_agent.qnetwork_local.conv1.weight.sum().item())

        # enable inference mode
        trained_agent.qnetwork_local.eval()

        # test and save results to disk
        test(env, trained_agent)
